{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard packages used to handle files\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "# commonly used library for data manipilation\n",
    "import pandas as pd\n",
    "\n",
    "# numerical\n",
    "import numpy as np\n",
    "\n",
    "# handle images - opencv\n",
    "import cv2\n",
    "\n",
    "# machine learning library\n",
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "\n",
    "#used to serialize python objects to disk and load them back to memory\n",
    "import pickle\n",
    "\n",
    "#plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# helper functions kindly provided for you by Matthias \n",
    "import helpers\n",
    "# specific helper functions for feature extraction\n",
    "import features\n",
    "\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'codebook_size': 1500, 'feature_count': 500, 'regularization': 2}\n",
      "['bobcat' 'chihuahua' 'collie' 'dalmatian' 'german_shepherd' 'leopard'\n",
      " 'lion' 'persian_cat' 'siamese_cat' 'tiger' 'wolf']\n",
      "Number of classes: 11\n"
     ]
    }
   ],
   "source": [
    "# filepath constants\n",
    "DATA_BASE_PATH = './'\n",
    "OUTPUT_PATH='./'\n",
    "\n",
    "\n",
    "DATA_TRAIN_PATH = os.path.join(DATA_BASE_PATH,'train')\n",
    "DATA_TEST_PATH = os.path.join(DATA_BASE_PATH,'test')\n",
    "\n",
    "FEATURE_BASE_PATH = os.path.join(OUTPUT_PATH,'features')\n",
    "FEATURE_TRAIN_PATH = os.path.join(FEATURE_BASE_PATH,'train')\n",
    "FEATURE_TEST_PATH = os.path.join(FEATURE_BASE_PATH,'test')\n",
    "\n",
    "PREDICTION_PATH = os.path.join(OUTPUT_PATH,'predictions')\n",
    "\n",
    "# filepatterns to write out features\n",
    "FILEPATTERN_DESCRIPTOR_TRAIN = os.path.join(FEATURE_TRAIN_PATH,'train_features_{}.pkl')\n",
    "FILEPATTERN_DESCRIPTOR_TEST = os.path.join(FEATURE_TEST_PATH,'test_features_{}.pkl')\n",
    "\n",
    "# create paths in case they don't exist:\n",
    "helpers.createPath(FEATURE_BASE_PATH)\n",
    "helpers.createPath(FEATURE_TRAIN_PATH)\n",
    "helpers.createPath(FEATURE_TEST_PATH)\n",
    "helpers.createPath(PREDICTION_PATH)\n",
    "\n",
    "\n",
    "\n",
    "#import yaml\n",
    "#with open(sys.argv[1]) as config_file:\n",
    "#    config = yaml.load(config_file)\n",
    "\n",
    "config = {\n",
    "    'codebook_size': 1500,\n",
    "    'feature_count': 500,\n",
    "    'regularization': 2\n",
    "}\n",
    "print(config)\n",
    "\n",
    "\n",
    "folder_paths = glob.glob(os.path.join(DATA_TRAIN_PATH,'*'))\n",
    "label_strings = np.sort(np.array([os.path.basename(path) for path in folder_paths]))\n",
    "num_classes = label_strings.shape[0]\n",
    "print(label_strings)\n",
    "print('Number of classes:', num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of encoded train images: 3556\n"
     ]
    }
   ],
   "source": [
    "descriptor_desired='sift_3000'\n",
    "with open(FILEPATTERN_DESCRIPTOR_TRAIN.format(descriptor_desired),'rb') as pkl_file_train:\n",
    "    train_features_from_pkl_sift = pickle.load(pkl_file_train)\n",
    "\n",
    "\n",
    "print('Number of encoded train images: {}'.format(len(train_features_from_pkl_sift)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training took 625.105090379715 seconds\n",
      "Created SIFT codebook\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clustered_codebook = helpers.createCodebook(train_features_from_pkl_sift, codebook_size = 2000)\n",
    "\n",
    "print('Created SIFT codebook')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 128)\n"
     ]
    }
   ],
   "source": [
    "print(train_features_from_pkl_sift[0].data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# encode all train images\n",
    "train_data_sift=[]\n",
    "train_labels_old=[]\n",
    "\n",
    "for i, image_features in enumerate(train_features_from_pkl_sift):\n",
    "    bow_feature_vector = helpers.encodeImage(image_features.data,clustered_codebook)\n",
    "    train_data_sift.append(bow_feature_vector)\n",
    "    train_labels_old.append(image_features.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bobcat', 'bobcat', 'bobcat', 'bobcat', 'bobcat', 'bobcat', 'bobcat', 'bobcat', 'bobcat', 'bobcat']\n",
      "[ 0  0  0 ... 10 10 10]\n"
     ]
    }
   ],
   "source": [
    "# use a labelencoder to obtain numerical labels\n",
    "label_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "label_encoder.fit(label_strings)\n",
    "print(train_labels_old[:10])\n",
    "train_labels = label_encoder.transform(train_labels_old)\n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for None\n",
      "Encoded all feature vectors\n",
      "[LibSVM][LibSVM][LibSVM][LibSVM][LibSVM]None\n",
      "[-1.34088801 -1.27669807 -1.33705193 -1.30252009 -1.40936519]\n",
      "Average validation accuracy:  -1.333304661312353 , stdev:  0.04474091876406463\n",
      "Loading data for daisy\n",
      "training took 28.593923091888428 seconds\n",
      "Created codebook for daisy\n",
      "Encoded all feature vectors\n",
      "[LibSVM][LibSVM][LibSVM][LibSVM][LibSVM]daisy\n",
      "[-1.32093443 -1.22392106 -1.28655728 -1.28849392 -1.30669476]\n",
      "Average validation accuracy:  -1.2853202911778214 , stdev:  0.03318645320324449\n",
      "Loading data for freak\n",
      "training took 23.360713958740234 seconds\n",
      "Created codebook for freak\n",
      "Encoded all feature vectors\n",
      "[LibSVM][LibSVM][LibSVM][LibSVM][LibSVM]freak\n",
      "[-1.35627179 -1.29948971 -1.33668092 -1.30495668 -1.37990088]\n",
      "Average validation accuracy:  -1.335459999040274 , stdev:  0.030443149264361938\n"
     ]
    }
   ],
   "source": [
    "for feature_name in [ None, 'daisy', 'freak']:\n",
    "    print('Loading data for', feature_name)\n",
    "    \n",
    "    if feature_name is None:\n",
    "        train_data_second = np.zeros_like(train_data_sift)\n",
    "    else:\n",
    "        with open(FILEPATTERN_DESCRIPTOR_TRAIN.format(feature_name),'rb') as pkl_file_train:\n",
    "            train_features_from_pkl_second = pickle.load(pkl_file_train)\n",
    "\n",
    "        clustered_codebook = helpers.createCodebook(train_features_from_pkl_second, codebook_size = 500)\n",
    "        print('Created codebook for', feature_name)\n",
    "\n",
    "        # encode all train images\n",
    "        train_data_second=[]\n",
    "\n",
    "        for i, image_features in enumerate(train_features_from_pkl_second):\n",
    "            bow_feature_vector = helpers.encodeImage(image_features.data,clustered_codebook)\n",
    "            train_data_second.append(bow_feature_vector)\n",
    "        \n",
    "    print('Encoded all feature vectors')\n",
    "    \n",
    "    train_data = np.concatenate([train_data_sift, train_data_second], axis=1)\n",
    "\n",
    "    simple_pipe = make_pipeline(SVC(random_state=0, probability=True, kernel='linear',C=2,verbose=True))\n",
    "\n",
    "    scores1 = cross_val_score(simple_pipe, train_data, train_labels, cv=5, scoring='neg_log_loss')\n",
    "    print(feature_name)\n",
    "    print(scores1)\n",
    "    print(\"Average validation accuracy: \",scores1.mean(),\", stdev: \",scores1.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

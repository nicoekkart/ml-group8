{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model\n",
    "\n",
    "Simon Schellaert\n",
    "\n",
    "This notebook implements the final model with tuned hyperparameters. The tuning of these hyperparameters was performed in `Experiments.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Required dependencies\n",
    "\n",
    "We start by including some packages that will be used in the remainder of the notebook. This prevents us from cluttering the other cells with imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard packages used to handle files\n",
    "import sys\n",
    "import os \n",
    "import glob\n",
    "import time\n",
    "\n",
    "# commonly used library for data manipilation\n",
    "import pandas as pd\n",
    "\n",
    "# numerical\n",
    "import numpy as np\n",
    "\n",
    "# handle images - opencv\n",
    "import cv2\n",
    "\n",
    "# machine learning library\n",
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "\n",
    "#used to serialize python objects to disk and load them back to memory\n",
    "import pickle\n",
    "\n",
    "#plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# helper functions kindly provided for you by Matthias \n",
    "import helpers\n",
    "# specific helper functions for feature extraction\n",
    "import features\n",
    "\n",
    "# tell matplotlib that we plot in a notebook\n",
    "%matplotlib notebook\n",
    "\n",
    "# filepath constants\n",
    "DATA_BASE_PATH = './'\n",
    "OUTPUT_PATH='./'\n",
    "\n",
    "DATA_TRAIN_PATH = os.path.join(DATA_BASE_PATH,'train')\n",
    "DATA_TEST_PATH = os.path.join(DATA_BASE_PATH,'test')\n",
    "\n",
    "FEATURE_BASE_PATH = os.path.join(OUTPUT_PATH,'features')\n",
    "FEATURE_TRAIN_PATH = os.path.join(FEATURE_BASE_PATH,'train')\n",
    "FEATURE_TEST_PATH = os.path.join(FEATURE_BASE_PATH,'test')\n",
    "\n",
    "PREDICTION_PATH = os.path.join(OUTPUT_PATH,'predictions')\n",
    "\n",
    "# filepatterns to write out features\n",
    "FILEPATTERN_DESCRIPTOR_TRAIN = os.path.join(FEATURE_TRAIN_PATH,'train_features_{}.pkl')\n",
    "FILEPATTERN_DESCRIPTOR_TEST = os.path.join(FEATURE_TEST_PATH,'test_features_{}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Augmenting the data set \n",
    "\n",
    "The model employs data augmentation in the form of horizontal flipped version of the images. To reduce the computation time while training, we flip all training images beforehand. The flipped version of `bobcat_0001.jpg` is saved as `bobcat_0001_flip.jpg`. To create these extra images, we use the ImageMagick convert utility. Concretely, we can generate flipped versions for all images by running the command below in each class folder.\n",
    "\n",
    "```sh\n",
    "for f in *.jpg; do convert $f -flop $(basename $f .jpg)_flip.jpg; done\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the train labels\n",
    "First, let's get the train labels. The train data is ordered in a way such that all images in a class are stored in a separate folder, thus we can simply get a string representation of the labels by using the folder names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_paths = glob.glob(os.path.join(DATA_TRAIN_PATH,'*'))\n",
    "label_strings = np.sort(np.array([os.path.basename(path) for path in folder_paths]))\n",
    "num_classes = label_strings.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths = dict((label_string, helpers.getImgPaths(os.path.join(DATA_TRAIN_PATH,label_string))) for label_string in label_strings)\n",
    "test_paths = helpers.getImgPaths(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading the BoVW image features\n",
    "\n",
    "To extract the features from the images (and their flipped versions), run the code in `ExtractFeatures.ipynb`. The extraction of the features is the analogous to the extraction provided in the example notebook. This time, however, we extract 3000 features from each images (see `features.py`). Once this extraction is done, we load the features here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(FILEPATTERN_DESCRIPTOR_TRAIN.format('sift'), 'rb') as pkl_file_train:\n",
    "    train_features_from_pkl_sift = pickle.load(pkl_file_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(FILEPATTERN_DESCRIPTOR_TRAIN.format('daisy'),'rb') as pkl_file_train:\n",
    "    train_features_from_pkl_daisy = pickle.load(pkl_file_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the codebook for both SIFT and DAISY based on the extracted features. Note that the hyperparameters chosen here are already optimized. This optimization was done in `Experiment.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training took 1473.2987508773804 seconds\n",
      "training took 215.16722512245178 seconds\n"
     ]
    }
   ],
   "source": [
    "clustered_codebook_sift = helpers.createCodebook(train_features_from_pkl_sift, codebook_size = 2000)\n",
    "clustered_codebook_daisy = helpers.createCodebook(train_features_from_pkl_daisy, codebook_size = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we construct a feature vector for all images for both the SIFT and DAISY features. To avoid duplicating code, we define two helpers function that will be used for both preprocessing the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_histogram_features(paths, number_of_bins = 10):\n",
    "    \"\"\" Returns a NumPy array containing the histogram feature given a list of image paths \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    for path in paths:\n",
    "        img = cv2.imread(path)\n",
    "        hist = cv2.calcHist([img], [0, 1, 2], None, [number_of_bins, number_of_bins, number_of_bins], 3 * [0, 256]).flatten()\n",
    "        features.append(hist / np.sum(hist))\n",
    "        \n",
    "    return np.array(features)\n",
    "\n",
    "def convert_features_to_bow(features, codebook):\n",
    "    \"\"\" Converts an array of features to a BoVW representation using the provided codebook \"\"\"\n",
    "    bow_vectors = []\n",
    "    \n",
    "    for feature in features:\n",
    "        bow_vector = helpers.encodeImage(feature.data, codebook)\n",
    "        bow_vectors.append(bow_vector)\n",
    "\n",
    "    return bow_vectors    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these helper functions, we construct the input data for each training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sift = convert_features_to_bow(train_features_from_pkl_sift, clustered_codebook_sift)\n",
    "train_data_daisy = convert_features_to_bow(train_features_from_pkl_daisy, clustered_codebook_daisy)\n",
    "train_data_hist = create_histogram_features([feature.path for feature in train_features_from_pkl_sift])\n",
    "\n",
    "train_data = np.concatenate([train_data_sift, train_data_daisy, train_data_hist], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we repeat this procedure for the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(FILEPATTERN_DESCRIPTOR_TEST.format('sift'),'rb') as pkl_file_test:\n",
    "    test_features_from_pkl_sift = pickle.load(pkl_file_test)\n",
    "\n",
    "with open(FILEPATTERN_DESCRIPTOR_TEST.format('daisy'),'rb') as pkl_file_test:\n",
    "    test_features_from_pkl_daisy = pickle.load(pkl_file_test)\n",
    "\n",
    "test_data_sift = convert_features_to_bow(test_features_from_pkl_sift, clustered_codebook_sift)\n",
    "test_data_daisy = convert_features_to_bow(test_features_from_pkl_daisy, clustered_codebook_daisy)\n",
    "test_data_hist = create_histogram_features([feature.path for feature in test_features_from_pkl_sift])\n",
    "\n",
    "test_data = np.concatenate([test_data_sift, test_data_daisy, test_data_hist], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we convert the string labels to numerical labels before feeding them to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "label_encoder.fit(label_strings)\n",
    "\n",
    "train_labels_raw = [image.label for image in train_features_from_pkl_sift]\n",
    "train_labels = label_encoder.transform(train_labels_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training the model\n",
    "For our naive sample submission we assume that every class occurs with the equal probability, thus we assign an equal probability over all classes to each image. <code>helpers.writePredictionsToCsv</code> can be used to write out predictions as a csv file ready to be submitted to the competition page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "classifier = SVC(random_state=0, probability=True, kernel='linear', C=0.9)\n",
    "classifier.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generating predictions for the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a trained model we can use to generate predictions. Generating a 2-dimensional array of probabilities is easy using the `predict_proba` function. Afterwards, we save the predictions in a CSV-file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.predict_proba(test_data)\n",
    "\n",
    "pred_file_path = os.path.join(PREDICTION_PATH, helpers.generateUniqueFilename('predictions','csv'))\n",
    "helpers.writePredictionsToCsv(predictions, pred_file_path, label_strings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

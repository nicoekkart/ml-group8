{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "Simon Schellaert\n",
    "\n",
    "This notebook is an adaptation of the provided notebook `Creating a visual bag-of-words.ipynb`. The logic is largely unchanged, the major difference is that we extract only the SIFT and DAISY features and extract 3000 instead of 500 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Required dependencies ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard packages used to handle files\n",
    "import sys\n",
    "import os \n",
    "import glob\n",
    "import time\n",
    "\n",
    "# commonly used library for data manipilation\n",
    "import pandas as pd\n",
    "\n",
    "# numerical\n",
    "import numpy as np\n",
    "\n",
    "# handle images - opencv\n",
    "import cv2\n",
    "\n",
    "# machine learning library\n",
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "\n",
    "#used to serialize python objects to disk and load them back to memory\n",
    "import pickle\n",
    "\n",
    "#plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# helper functions kindly provided for you by Matthias \n",
    "import helpers\n",
    "# specific helper functions for feature extraction\n",
    "import features\n",
    "\n",
    "# tell matplotlib that we plot in a notebook\n",
    "%matplotlib notebook\n",
    "\n",
    "\n",
    "# filepath constants\n",
    "DATA_BASE_PATH = './'\n",
    "OUTPUT_PATH='./'\n",
    "\n",
    "\n",
    "DATA_TRAIN_PATH = os.path.join(DATA_BASE_PATH,'train')\n",
    "DATA_TEST_PATH = os.path.join(DATA_BASE_PATH,'test')\n",
    "\n",
    "FEATURE_BASE_PATH = os.path.join(OUTPUT_PATH,'features')\n",
    "FEATURE_TRAIN_PATH = os.path.join(FEATURE_BASE_PATH,'train')\n",
    "FEATURE_TEST_PATH = os.path.join(FEATURE_BASE_PATH,'test')\n",
    "\n",
    "PREDICTION_PATH = os.path.join(OUTPUT_PATH,'predictions')\n",
    "\n",
    "# filepatterns to write out features\n",
    "FILEPATTERN_DESCRIPTOR_TRAIN = os.path.join(FEATURE_TRAIN_PATH,'train_features_{}.pkl')\n",
    "FILEPATTERN_DESCRIPTOR_TEST = os.path.join(FEATURE_TEST_PATH,'test_features_{}.pkl')\n",
    "\n",
    "# create paths in case they don't exist:\n",
    "helpers.createPath(FEATURE_BASE_PATH)\n",
    "helpers.createPath(FEATURE_TRAIN_PATH)\n",
    "helpers.createPath(FEATURE_TEST_PATH)\n",
    "helpers.createPath(PREDICTION_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data and the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_paths = glob.glob(os.path.join(DATA_TRAIN_PATH,'*'))\n",
    "label_strings = np.sort(np.array([os.path.basename(path) for path in folder_paths]))\n",
    "num_classes = label_strings.shape[0]\n",
    "\n",
    "train_paths = dict((label_string, helpers.getImgPaths(os.path.join(DATA_TRAIN_PATH,label_string))) for label_string in label_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extracting the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted before, we only use the SIFT and DAISY features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptor_dict = {\n",
    "    'daisy': features.extractDAISYCallback,\n",
    "    'sift': features.extractSIFTCallback\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have selected all the features we want to extract in the dictionary above we can proceed by extracting all desired features from the train data and writing them to the harddrive for later using the pickle module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_descriptor_dict = descriptor_dict.copy()\n",
    "\n",
    "# if the corresponding files already exist, do not extract them again\n",
    "train_descriptor_dict = dict((key,value) for (key,value) in descriptor_dict.items() if not os.path.isfile(FILEPATTERN_DESCRIPTOR_TRAIN.format(key)))\n",
    "\n",
    "if len(train_descriptor_dict) > 0: \n",
    "    train_features = []\n",
    "    train_labels = []\n",
    "    \n",
    "    # convert train images\n",
    "    train_features_by_descriptor = dict((key,[]) for (key,value) in train_descriptor_dict.items())\n",
    "    \n",
    "    for label_string in label_strings:\n",
    "        print('extracting train features for class {} :'.format(label_string))\n",
    "\n",
    "        extracted_features = features.extractFeatures(train_paths[label_string],train_descriptor_dict, label_string)\n",
    "\n",
    "        # append descriptors of corresponding label to correct descriptor list \n",
    "        for key in train_features_by_descriptor.keys():\n",
    "            train_features_by_descriptor[key]+=extracted_features[key]\n",
    "  \n",
    "    for descriptor_key in train_features_by_descriptor.keys():\n",
    "        with open(FILEPATTERN_DESCRIPTOR_TRAIN.format(descriptor_key),'wb') as pkl_file_train:\n",
    "            pickle.dump(train_features_by_descriptor[descriptor_key], pkl_file_train, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we do the same for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_paths = helpers.getImgPaths(DATA_TEST_PATH)\n",
    "test_descriptor_dict = descriptor_dict.copy()\n",
    "\n",
    "if len(test_descriptor_dict) > 0: \n",
    "    test_features = []\n",
    "    \n",
    "    print('extracting test features:') \n",
    "    test_features_by_descriptor = features.extractFeatures(test_paths,test_descriptor_dict, None) \n",
    "    \n",
    "    for descriptor_key in test_features_by_descriptor.keys():\n",
    "        with open(FILEPATTERN_DESCRIPTOR_TEST.format(descriptor_key),'wb') as pkl_file_test:\n",
    "            pickle.dump(test_features_by_descriptor[descriptor_key], pkl_file_test, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
